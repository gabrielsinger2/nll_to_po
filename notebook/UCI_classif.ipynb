{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from nll_to_po.models.dn_policy import MLPPolicy, MLPPolicy_Full_Cov\n",
    "from nll_to_po.training.utils import train_single_policy, setup_logger\n",
    "import nll_to_po.training.loss as L\n",
    "import nll_to_po.training.reward as R\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.despine()\n",
    "\n",
    "logger = logging.getLogger(\"wandb\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"  # Suppress WandB output\n",
    "import pretty_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x126b4c110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install scikit-learn pandas matplotlib seaborn torch --quiet\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_wine, load_breast_cancer, load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uci(dataset=\"wine\", test_size=0.2, val_size=0.2, batch_size=256, standardize=True):\n",
    "    if dataset == \"wine\":\n",
    "        data = load_wine()\n",
    "    if dataset == \"wine\":\n",
    "        data = load_wine()\n",
    "    elif dataset == \"iris\":\n",
    "        data = load_iris()\n",
    "    elif dataset == \"breast_cancer\":\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset == \"load_digits\":\n",
    "        data = load_digits()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "    X, y = data.data, data.target\n",
    "    if standardize:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    # train / (val+test)\n",
    "    X_tr, X_tt, y_tr, y_tt = train_test_split(X, y, test_size=test_size, stratify=y, random_state=0)\n",
    "    # split val from train\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=val_size, stratify=y_tr, random_state=0)\n",
    "    #conversion en tensor type pour le donner au dataloader \n",
    "    X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    X_tt = torch.tensor(X_tt, dtype=torch.float32)\n",
    "    y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_tt = torch.tensor(y_tt, dtype=torch.long)\n",
    "\n",
    "    tr_loader  = DataLoader(TensorDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n",
    "    #on ne shuffle pas la val et le test\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "    tst_loader = DataLoader(TensorDataset(X_tt, y_tt), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    meta = {\"input_dim\": X.shape[1], \"num_classes\": len(np.unique(y))}\n",
    "    return tr_loader, val_loader, tst_loader, meta\n",
    "\n",
    "train_loader, val_loader, test_loader, meta = load_uci(\"wine\", batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, meta=load_uci(dataset=\"wine\", test_size=0.2, val_size=0.2, batch_size=256, standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up $\\widehat{Y}$ c'est le one hot encoding des labels Y et je travaille avec les one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_trace_sigma_onehot(train_loader, num_classes: int) -> float:\n",
    "    ys = []\n",
    "    for _, yb in train_loader:\n",
    "        ys.append(yb)\n",
    "    y = torch.cat(ys, dim=0)                                # (N,)\n",
    "    Y = F.one_hot(y, num_classes=num_classes).float()       # (N,C)\n",
    "    Yc = Y - Y.mean(dim=0, keepdim=True)                    # (N,C)\n",
    "    Sigma = (Yc.T @ Yc) / max(Y.shape[0]-1, 1)              # (C,C)\n",
    "    return float(torch.trace(Sigma))\n",
    "\n",
    "def beta_star_from_data(train_loader, entropy_weight: float, num_classes: int) -> float:\n",
    "    tr = estimate_trace_sigma_onehot(train_loader, num_classes=num_classes)\n",
    "    return float(entropy_weight * num_classes / (2.0 * max(tr, 1e-12)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MulticlassLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)               # (B, C)\n",
    "        probs = F.softmax(logits, dim=-1)      # (B, C)\n",
    "        return logits, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_accuracy(policy: nn.Module, data_loader: DataLoader) -> float:\n",
    "    policy.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in data_loader:\n",
    "        logits = policy(xb)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total   += yb.numel()\n",
    "    return correct / max(total, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate_accuracy_multi_regression(policy: nn.Module, data_loader: DataLoader)-> float:\n",
    "    policy.eval()\n",
    "    correct, total= 0,0\n",
    "    for xb,yb in data_loader:\n",
    "        #yb est un vecteur [num_samples] a valeur dans les labes [0;1,...,C-1]\n",
    "        logits, prob_predit=policy(xb)\n",
    "        one_hot_pred=prob_predit.argmax(dim=-1)\n",
    "        #print(f'prob shape {prob_predit.shape}')\n",
    "        #print(f'one hot true shape {yb.shape}')\n",
    "        y_prob=prob_predit.agrmax(dim=-1)\n",
    "        correct+=(y_prob==yb).sum().item()\n",
    "        total+=yb.numel()\n",
    "    if total==1:\n",
    "        return \"One element in the dataloader\"\n",
    "    else:\n",
    "        return correct/max(total,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_wine()['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, meta = load_uci( standardize=True, batch_size=300\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36])\n",
      "torch.Size([36, 3])\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for x_test, y_test in test_loader:\n",
    "    print(y_test.shape)\n",
    "\n",
    "    print(F.one_hot(y_test).shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_dataset(\n",
    "    dataset_name: str,\n",
    "    n_updates: int = 10,\n",
    "    n_experiments: int = 5,\n",
    "    batch_size: int = 128,\n",
    "    learning_rate: float = 1e-4,\n",
    "    entropy_weight: float = 1e-3):\n",
    "    \n",
    "    train_loader, val_loader, test_loader, meta = load_uci(\n",
    "        dataset=dataset_name, batch_size=batch_size, standardize=True\n",
    "    )\n",
    "    C = meta[\"num_classes\"]\n",
    "    D_in = meta[\"input_dim\"]\n",
    "\n",
    "    beta_star = beta_star_from_data(train_loader, entropy_weight=entropy_weight, num_classes=C)\n",
    "    beta_list = [1, beta_star]\n",
    "\n",
    "    curves = []   \n",
    "    tests  = []  \n",
    "\n",
    "    for rep in range(n_experiments):\n",
    "        #policy = MLPClassifier(D_in, C)\n",
    "        policy=MulticlassLogisticRegression(D_in,C)\n",
    "        loss_fn = L.NLL_Classification()\n",
    "\n",
    "        trained_policy, train_metrics, val_metrics = train_single_policy(\n",
    "            policy=policy,\n",
    "            train_dataloader=train_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            loss_function=loss_fn,\n",
    "            n_updates=n_updates,\n",
    "            learning_rate=learning_rate,\n",
    "            wandb_run=None,\n",
    "            tensorboard_writer=None,\n",
    "            logger=None,\n",
    "        )\n",
    "\n",
    "        df_tr = pd.DataFrame(train_metrics).reset_index().rename(columns={\"index\":\"epoch\"})\n",
    "        df_tr[\"split\"] = \"train\"; df_tr[\"method\"] = \"NLL\"; df_tr[\"beta\"] = np.nan; df_tr[\"rep\"] = rep\n",
    "        df_val = pd.DataFrame(val_metrics).reset_index().rename(columns={\"index\":\"epoch\"})\n",
    "        df_val[\"split\"] = \"val\"; df_val[\"method\"] = \"NLL\"; df_val[\"beta\"] = np.nan; df_val[\"rep\"] = rep\n",
    "        curves += [df_tr, df_val]\n",
    "\n",
    "        #test_acc = evaluate_accuracy(policy, test_loader)\n",
    "        test_acc=evaluate_accuracy_multi_regression(trained_policy, test_loader)\n",
    "        tests.append({\"dataset\": dataset_name, \"method\": \"NLL\", \"beta\": np.nan, \"rep\": rep, \"test_accuracy\": test_acc})\n",
    "\n",
    "    for beta in beta_list:\n",
    "        U = torch.eye(C) * float(beta)\n",
    "        reward = R.OneHotMahalanobis(U, num_classes=C)   # reward\n",
    "\n",
    "        for rep in range(n_experiments):\n",
    "            policy = MulticlassLogisticRegression(D_in, C)\n",
    "            loss_fn = L.PO_Entropy_Classification(\n",
    "                reward_fn=reward,\n",
    "                n_generations=50,       \n",
    "                use_rsample=False,\n",
    "                reward_transform=\"normalize\",\n",
    "                entropy_weight=entropy_weight,\n",
    "            )\n",
    "\n",
    "            beta_trained_policy, train_metrics, val_metrics = train_single_policy(\n",
    "                policy=policy,\n",
    "                train_dataloader=train_loader,\n",
    "                val_dataloader=val_loader,\n",
    "                loss_function=loss_fn,\n",
    "                n_updates=n_updates,\n",
    "                learning_rate=learning_rate,\n",
    "                wandb_run=None,\n",
    "                tensorboard_writer=None,\n",
    "                logger=None,\n",
    "            )\n",
    "\n",
    "            df_tr = pd.DataFrame(train_metrics).reset_index().rename(columns={\"index\":\"epoch\"})\n",
    "            df_tr[\"split\"] = \"train\"; df_tr[\"method\"] = \"PO_Entropy\"; df_tr[\"beta\"] = beta; df_tr[\"rep\"] = rep\n",
    "            df_val = pd.DataFrame(val_metrics).reset_index().rename(columns={\"index\":\"epoch\"})\n",
    "            df_val[\"split\"] = \"val\"; df_val[\"method\"] = \"PO_Entropy\"; df_val[\"beta\"] = beta; df_val[\"rep\"] = rep\n",
    "            curves += [df_tr, df_val]\n",
    "\n",
    "            #test_acc = evaluate_accuracy(policy, test_loader)\n",
    "            #Comment etre sur que le modele policy est bien celui last epoch ?? \n",
    "            test_acc = evaluate_accuracy_multi_regression(beta_trained_policy, test_loader)\n",
    "            tests.append({\"dataset\": dataset_name, \"method\": \"PO_Entropy\", \"beta\": beta, \"rep\": rep, \"test_accuracy\": test_acc})\n",
    "\n",
    "    curves_df = pd.concat(curves, ignore_index=True)\n",
    "    tests_df  = pd.DataFrame(tests)\n",
    "    curves_df[\"is_beta_star\"] = curves_df[\"beta\"].apply(lambda b: isinstance(b, float) and abs(b - beta_star) < 1e-12)\n",
    "    tests_df[\"is_beta_star\"]  = tests_df[\"beta\"].apply(lambda b: isinstance(b, float) and abs(b - beta_star) < 1e-12)\n",
    "    return curves_df, tests_df, beta_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves_for_dataset(curves_df: pd.DataFrame, dataset_name: str, beta_star: float):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "    # helpers\n",
    "    def make_label(row):\n",
    "        if row[\"method\"] == \"NLL\":\n",
    "            return \"NLL\"\n",
    "        if pd.isna(row[\"beta\"]):\n",
    "            return \"PO (β=NA)\"\n",
    "        if abs(row[\"beta\"] - beta_star) < 1e-12:\n",
    "            return \"PO (β*)\"\n",
    "        return f\"PO (β={row['beta']:.3g})\"\n",
    "\n",
    "    curves_df = curves_df.copy()\n",
    "    curves_df[\"label\"] = curves_df.apply(make_label, axis=1)\n",
    "\n",
    "    # colors: NLL blue, β* black, others red\n",
    "    palette_map = {}\n",
    "    for lab in curves_df[\"label\"].unique():\n",
    "        if lab == \"NLL\":\n",
    "            palette_map[lab] = \"#1f77b4\"\n",
    "        elif lab == \"PO (β*)\":\n",
    "            palette_map[lab] = \"black\"\n",
    "        else:\n",
    "            palette_map[lab] = \"red\"\n",
    "\n",
    "    # TRAIN\n",
    "    sub = curves_df[curves_df[\"split\"] == \"train\"]\n",
    "    sns.lineplot(data=sub, x=\"epoch\", y=\"accuracy\", hue=\"label\",\n",
    "                 errorbar=(\"ci\", 95), ax=ax[0], palette=palette_map, legend=False)\n",
    "    ax[0].set_title(f\"{dataset_name}: Train accuracy vs epoch\")\n",
    "    ax[0].set_xlabel(\"epoch\"); ax[0].set_ylabel(\"accuracy\")\n",
    "\n",
    "    # VAL\n",
    "    sub = curves_df[curves_df[\"split\"] == \"val\"]\n",
    "    sns.lineplot(data=sub, x=\"epoch\", y=\"accuracy\", hue=\"label\",\n",
    "                 errorbar=(\"ci\", 95), ax=ax[1], palette=palette_map, legend=True)\n",
    "    ax[1].set_title(f\"{dataset_name}: Val accuracy vs epoch\")\n",
    "    ax[1].set_xlabel(\"epoch\"); ax[1].set_ylabel(\"accuracy\")\n",
    "    ax[1].legend(title=\"method\", frameon=False, loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_table(tests_df: pd.DataFrame, dataset_name: str):\n",
    "    keep = (tests_df[\"method\"] == \"PO_Entropy\") | (tests_df[\"method\"] == \"NLL\")\n",
    "    df = tests_df[keep].copy()\n",
    "\n",
    "    grouped = (df\n",
    "        .groupby([\"dataset\", \"method\", \"is_beta_star\"], dropna=False)[\"test_accuracy\"]\n",
    "        .agg([\"mean\", \"std\", \"count\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    def beta_label(row):\n",
    "        if row[\"method\"] == \"NLL\":\n",
    "            return \"—\"\n",
    "        return \"β*\" if row[\"is_beta_star\"] else \"β=1\"\n",
    "\n",
    "    grouped[\"beta_label\"] = grouped.apply(beta_label, axis=1)\n",
    "    grouped = grouped[[\"dataset\", \"method\", \"beta_label\", \"mean\", \"std\", \"count\"]]\n",
    "    grouped = grouped[grouped[\"dataset\"] == dataset_name]\n",
    "\n",
    "    print(f\"Test accuracy summary — {dataset_name}\")\n",
    "    display(grouped.style.format({\"mean\":\"{:.4f}\", \"std\":\"{:.4f}\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 300/300 [00:00<00:00, 1237.01it/s]\n",
      "Training epochs: 100%|██████████| 300/300 [00:00<00:00, 1384.09it/s]\n",
      "Training epochs: 100%|██████████| 300/300 [00:00<00:00, 1387.30it/s]\n",
      "Training epochs: 100%|██████████| 300/300 [00:00<00:00, 1388.16it/s]\n",
      "Training epochs: 100%|██████████| 300/300 [00:00<00:00, 1390.41it/s]\n",
      "Training epochs:   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m all_tests  = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     curves_df, tests_df, bstar = \u001b[43mrun_one_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_updates\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_experiments\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mentropy_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     plot_curves_for_dataset(curves_df, ds, bstar)\n\u001b[32m     16\u001b[39m     show_test_table(tests_df, ds)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mrun_one_dataset\u001b[39m\u001b[34m(dataset_name, n_updates, n_experiments, batch_size, learning_rate, entropy_weight)\u001b[39m\n\u001b[32m     53\u001b[39m policy = MulticlassLogisticRegression(D_in, C)\n\u001b[32m     54\u001b[39m loss_fn = L.PO_Entropy_Classification(\n\u001b[32m     55\u001b[39m     reward_fn=reward,\n\u001b[32m     56\u001b[39m     n_generations=\u001b[32m50\u001b[39m,       \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     entropy_weight=entropy_weight,\n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m beta_trained_policy, train_metrics, val_metrics = \u001b[43mtrain_single_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_updates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensorboard_writer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m df_tr = pd.DataFrame(train_metrics).reset_index().rename(columns={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     75\u001b[39m df_tr[\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m; df_tr[\u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mPO_Entropy\u001b[39m\u001b[33m\"\u001b[39m; df_tr[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m] = beta; df_tr[\u001b[33m\"\u001b[39m\u001b[33mrep\u001b[39m\u001b[33m\"\u001b[39m] = rep\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Reinforcment_Learning_Huawei/nll_to_po/src/nll_to_po/training/utils.py:76\u001b[39m, in \u001b[36mtrain_single_policy\u001b[39m\u001b[34m(policy, train_dataloader, loss_function, n_updates, learning_rate, val_dataloader, wandb_run, tensorboard_writer, logger, adam_weight_decay)\u001b[39m\n\u001b[32m     74\u001b[39m batch_count = \u001b[32m0\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     loss, metrics = \u001b[43mloss_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     optimizer.zero_grad()\n\u001b[32m     79\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Reinforcment_Learning_Huawei/nll_to_po/src/nll_to_po/training/loss.py:400\u001b[39m, in \u001b[36mPO_Entropy_Classification.compute_loss\u001b[39m\u001b[34m(self, policy, X, y)\u001b[39m\n\u001b[32m    398\u001b[39m pred = probs.argmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    399\u001b[39m pred_one_hot=F.one_hot(pred)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m nll = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m#print(f'PRED: {pred}')\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m#print(f'Y: {y}')\u001b[39;00m\n\u001b[32m    404\u001b[39m acc  = (pred == y).float().mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nllpo/lib/python3.12/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "datasets = [\"wine\", \"iris\", \"breast_cancer\" , \"load_digits\"]  \n",
    "\n",
    "all_curves = []\n",
    "all_tests  = []\n",
    "\n",
    "for ds in datasets:\n",
    "    curves_df, tests_df, bstar = run_one_dataset(\n",
    "        dataset_name=ds,\n",
    "        n_updates=300,\n",
    "        n_experiments=5,     \n",
    "        batch_size=128,\n",
    "        learning_rate=1e-4,\n",
    "        entropy_weight=1e-1, \n",
    "    )\n",
    "    plot_curves_for_dataset(curves_df, ds, bstar)\n",
    "    show_test_table(tests_df, ds)\n",
    "\n",
    "    curves_df[\"dataset\"] = ds\n",
    "    tests_df[\"dataset\"]  = ds\n",
    "    all_curves.append(curves_df)\n",
    "    all_tests.append(tests_df)\n",
    "\n",
    "df_curves_all = pd.concat(all_curves, ignore_index=True)\n",
    "df_tests_all  = pd.concat(all_tests,  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=torch.eye(5)\n",
    "n_generations=5\n",
    "dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "samples = dist.sample((n_generations,))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 2, 3, 2],\n",
       "        [0, 2, 2, 2, 3],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [1, 3, 1, 1, 1],\n",
       "        [4, 1, 3, 3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp    = dist.log_prob(samples)                 # (G, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9048, -1.9048, -0.9048, -0.9048, -1.9048],\n",
       "        [-0.9048, -1.9048, -0.9048, -1.9048, -1.9048],\n",
       "        [-0.9048, -0.9048, -0.9048, -0.9048, -0.9048],\n",
       "        [-1.9048, -1.9048, -1.9048, -1.9048, -1.9048],\n",
       "        [-1.9048, -0.9048, -1.9048, -0.9048, -0.9048]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "X,y=load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(random_state=0, penalty=None)\n",
    "model.fit(X_train,y_train)\n",
    "score=model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de la regression logistic sur wine dataset (sklearn) est de :1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Le score de la regression logistic sur wine dataset (sklearn) est de :{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predit par le modele [0 0 0]\n",
      "donnee de terrain: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'predit par le modele {model.predict(X[:3,:])}')\n",
    "print(f'donnee de terrain: {y[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nllpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
